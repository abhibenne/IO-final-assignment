{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.request\n",
    "import time\n",
    "import bs4 as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Due to the proxy error i experienced i had to hardcode the Names of the html files which i would have otherwise done using\n",
    "# a urlib request and finding all the links and adding the names to a list.\n",
    "list=['Albury',\n",
    "'BadgerysCreek',\n",
    "'Cobar',\n",
    "'CoffsHarbour',\n",
    "'Moree',\n",
    "'Newcastle',\n",
    "'NorahHead',\n",
    "'NorfolkIsland',\n",
    "'Penrith',\n",
    "'Richmond',\n",
    "'Sydney',\n",
    "'SydneyAirport',\n",
    "'WaggaWagga',\n",
    "'Williamtown',\n",
    "'Wollongong',\n",
    "'Canberra',\n",
    "'Tuggeranong',\n",
    "'MountGinini',\n",
    "'Ballarat',\n",
    "'Bendigo',\n",
    "'Sale',\n",
    "'MelbourneAirport',\n",
    "'Melbourne',\n",
    "'Mildura',\n",
    "'Nhil',\n",
    "'Portland',\n",
    "'Watsonia',\n",
    "'Dartmoor',\n",
    "'Brisbane',\n",
    "'Cairns',\n",
    "'GoldCoast',\n",
    "'Townsville',\n",
    "'Adelaide',\n",
    "'MountGambier',\n",
    "'Nuriootpa',\n",
    "'Woomera',\n",
    "'Albany',\n",
    "'Witchcliffe','PearceRAAF','PerthAirport','Perth','SalmonGums','Walpole','Hobart','Launceston','AliceSprings','Darwin','Katherine','Uluru']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i have used csv module in this task to facilitate copying all data to a csv file.\n",
    "\n",
    "import csv\n",
    "csvfile=open('All.csv','w+',newline='')\n",
    "writer=csv.writer(csvfile)\n",
    "writer.writerow(['Date',\n",
    " 'Location',\n",
    " 'MinTemp',\n",
    " 'MaxTemp',\n",
    " 'Rainfall',\n",
    " 'Evaporation',\n",
    " 'Sunshine',\n",
    " 'WindGustDir',\n",
    " 'WindGustSpeed',\n",
    " 'WindDir9am',\n",
    " 'WindDir3pm',\n",
    " 'WindSpeed9am',\n",
    " 'WindSpeed3pm',\n",
    " 'Humidity9am',\n",
    " 'Humidity3pm',\n",
    " 'Pressure9am',\n",
    " 'Pressure3pm',\n",
    " 'Cloud9am',\n",
    " 'Cloud3pm',\n",
    " 'Temp9am',\n",
    " 'Temp3pm',\n",
    " 'RainToday',\n",
    " 'RISK_MM',\n",
    " 'RainTomorrow'])\n",
    "for j in list:\n",
    "    html=urllib.request.urlopen('file:///D:/Users/abhib/Desktop/weather%20data/Weather-Data-master/'+j+'.html').read()\n",
    "    soup=bs.BeautifulSoup(html,'lxml')\n",
    "    rows=soup.find_all('tr')\n",
    "    labels=[]\n",
    "    labels=str(rows[0].text).split('\\n')\n",
    "    data=[]\n",
    "    for row in soup.find_all('td'):\n",
    "        data.append(str(row.text))\n",
    "    for i in range(0,len(data),24):\n",
    "        if (i+24)==len(data):\n",
    "            break\n",
    "        writer.writerow([data[i],data[i+1],data[i+2],data[i+3],data[i+4],data[i+5],data[i+6],data[i+7],data[i+8],data[i+9],data[i+10],data[i+11],data[i+12],data[i+13],data[i+14],data[i+15],data[i+16],data[i+17],data[i+18],data[i+19],data[i+20],data[i+21],data[i+22],data[i+23]])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#csvfile is now ready with all the data in csv file called All.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
